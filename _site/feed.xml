<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Emute Lab</title>
    <description>A Music Informatics and Performance Technologies Lab based in the School of Media, Film and Music at the University of Sussex</description>
    <link>http://www.emutelab.org/</link>
    <atom:link href="http://www.emutelab.org/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 04 Feb 2022 10:20:23 +0000</pubDate>
    <lastBuildDate>Fri, 04 Feb 2022 10:20:23 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Emute Lab 6 @ The Rose Hill</title>
        <description>&lt;p&gt;&lt;img src=&quot;/../../img/emutelab6ws.png&quot; alt=&quot;emutelab6!&quot; /&gt;&lt;/p&gt;
&lt;h1&gt;Livecoding Workshops&lt;/h1&gt;
&lt;h3&gt;11:00 - 16:00&lt;/h3&gt;
&lt;p&gt;Facebook Event TBA &lt;br /&gt; Tickets TBA: £7 or £4 (concession/student)&lt;/p&gt;

&lt;p&gt;Dimitris Kyriakoudis - Live Coding with TimeLines: Musical Maths&lt;/p&gt;

&lt;p&gt;Lizzie Wilson - Live Coding with TidalCycles&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;/../../img/emutelab6.png&quot; alt=&quot;emutelab6!&quot; /&gt;&lt;/p&gt;
&lt;h1&gt;Experimental A/V Performances&lt;/h1&gt;
&lt;h3&gt;19:00 - 22:30&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.facebook.com/events/1090542748411764/&quot;&gt;Facebook Event&lt;/a&gt; &lt;br /&gt; &lt;a href=&quot;https://www.ticketsource.co.uk/emute-lab/emute-lab-6-gig/e-mzglpb&quot;&gt;Tickets&lt;/a&gt;: £7 or £4 (concession/student)&lt;/p&gt;

&lt;h3&gt;Digital Selves&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.twitter.com/@dgtlselves&quot;&gt;@dgtlselves&lt;/a&gt;&lt;/p&gt;

&lt;iframe width=&quot;220&quot; height=&quot;100&quot; src=&quot;https://www.youtube.com/embed/VPeX3hzsiO0&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3&gt;Joana Chicau and Jonothan Reus&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.twitter.com/@bchicau&quot;&gt;@bchicau &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.twitter.com/@_jchaim&quot;&gt;@_jchaim&lt;/a&gt;&lt;/p&gt;

&lt;iframe width=&quot;220&quot; height=&quot;100&quot; src=&quot;https://www.youtube.com/embed/zMydMePetkE&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3&gt;w1n5t0n&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.twitter.com/@lmw1n5t0n&quot;&gt;@lmw1n5t0n&lt;/a&gt;&lt;/p&gt;

&lt;iframe width=&quot;220&quot; height=&quot;100&quot; src=&quot;https://www.youtube.com/embed/dsHnWE6_JbE&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3&gt;Steve Symons and Sam Bilbow&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.twitter.com/@stevemuio&quot;&gt;@stevemuio&lt;/a&gt;&lt;/p&gt;

&lt;iframe width=&quot;220&quot; height=&quot;100&quot; src=&quot;https://www.youtube.com/embed/3MBnnZnGak8&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href=&quot;https://www.twitter.com/@sambilbow&quot;&gt;@sambilbow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Sam is a Doctoral Researcher at Sussex working with augmented reality as a medium for gestural and musical expression.&lt;/p&gt;
&lt;iframe width=&quot;220&quot; height=&quot;100&quot; src=&quot;https://www.youtube.com/embed/gY2QtK907cU?start=92&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href=&quot;https://www.therosehill.co.uk&quot;&gt;https://www.therosehill.co.uk&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate>
        <link>http://www.emutelab.org/blog/emutelab6</link>
        <guid isPermaLink="true">http://www.emutelab.org/blog/emutelab6</guid>
        
        <category>emutelab</category>
        
        <category>event</category>
        
        <category>experimental</category>
        
        <category>livecoding</category>
        
        <category>algorithmic</category>
        
        <category>augmented reality</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Machines on Stage: Exploring the World of Robot Opera</title>
        <description>&lt;h1 id=&quot;machines-on-stage-exploring-the-world-of-robot-opera&quot;&gt;Machines on Stage: Exploring the World of Robot Opera&lt;/h1&gt;

&lt;p&gt;A number of Emute Lab members are interested in robotics and AI. A key area where music, robotics and AI meet is Robot Opera. Robot Opera in its simplest sense is an opera which has robots in it. You could argue that in some sense this idea goes way back, as automata and stage machinery have been part of theatre and opera for a long time (for example of the latter, see Corneille’s Andromède, described as a tragedy represented by machines). The word ‘robot’ itself has its origins in theatre - a play by Czech playwright Karel Çapek, Rossum’s Universal Robots’ coined the term in 1921.&lt;/p&gt;

&lt;p&gt;What’s behind the current wave of operatic interest in robots? There’s a broader context at work, in both theatre and in music. At the Performing Robots Conference in Utrecht in 2019, the point was repeatedly made that theatre could be a ‘sandbox’ or ‘test bed’ for human robot interaction, with much to be learnt about how humans see themselves (through creating robotic doubles) and about how we can imagine our future relationships with robots.&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/379722275?h=8fdba7dd82&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&quot;https://vimeo.com/379722275&quot;&gt;Performing Robots Conference 2019, Utrecht (23rd-25th May)&lt;/a&gt; from &lt;a href=&quot;https://vimeo.com/user106402189&quot;&gt;TiM&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;On the music side, an example of robotic musical innovation can be found at Georgia Tech, where researchers have developed Shimon, a marimba playing robot who can improvise jazz with human musicians. Shimon is currently be trained to sing and write songs collaboratively with humans.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/BOck0kPtlfk&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In robot opera itself, there’s a wide variety of practice out there, featuring both humanoid and non-humanoid robots, singing and performing in a range of different styles. One of the most high profile of these is Todd Machover’s Death and the Powers (2010) produced at MIT. An iconic work of the modern genre, it featured eight non-humanoid robots and a human cast, including video and complex moving stage scenery.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/T0ZsUAiAcXE&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Interestingly, the non-humanoid robots sounded, at time, a lot like humans.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Myd2DdSxUEk&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Wade Marynowski’s Robot Opera (2015) by contrast featured non-humanoid robots that sounded very non-human, exploring the tropes of electronic music, and combining sound, movement, sculpture and interactivity, blurring boundaries between set, character, performers and audience.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/eOgKzrsgb2s&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In a more traditional staging, a humanoid robot in Gob Squad’s 2015 robot opera My Square Lady was trained to conduct and sing as humans do, in a quest to learn about human emotion.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/49rJMgJY1CU&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In the Royal Opera House’s 2015 commission Glare, a human singer played a robot character, who was trying to ‘pass’ as human.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/FZzS2A5Vu6U&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Clearly this opera did not have an actual robot on stage, nevertheless such scenarios allow for serious exploration of human/robot identities and relationships. Who gets to decide who is human, and how important are relationships in these definitions? On stage, if a ‘real’ robot performs, how are we relating to that as an audience, compared to a human performer pretending to be a robot? These sorts of questions are also being explored extensively in France, where the Ecole Universitaire de Recherche hosts an ongoing international webinar entitled Robots on Stage. Interdisciplinary Convergences between Robotics and Theatre.&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/555319614?h=3d27ba3f29&amp;amp;byline=0&amp;amp;portrait=0&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&quot;https://vimeo.com/555319614&quot;&gt;Conf&amp;eacute;rence - Robots and theatre : creations, challenges and research prospects&lt;/a&gt; from &lt;a href=&quot;https://vimeo.com/eurartec&quot;&gt;EUR ArTeC&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;robot-opera--sussex&quot;&gt;Robot Opera @ Sussex&lt;/h2&gt;

&lt;p&gt;At the Centre for Research in Opera and Music Theatre (CROMT) we started a robot opera project in 2017, and we have had and additional two research events in 2019 and 2021. Our focus is on:
    • voice, embodiment and performance
    • creative exploration of human robot interaction
    • scholarly reflection on robotic otherness and human/robot hybridity, including ethical dimensions of our relationships with robots 
    • how new technologies create new forms of expression and new forms for opera, including how explorations of robotic vocal virtuosity might feed back into human music making&lt;/p&gt;

&lt;p&gt;How would a robot sing if it sang like a robot, rather than emulating a human? What would it sound like if its voice came from its own materiality and physicality? How do we read a robot’s presence on stage? Can a robot perform, or is it in some sense, always performing? Music software can enable computers to ‘improvise’ musically - if a robot is really just a computer that can move around, how might that movement be used to influence its vocal production?    How could embodiment feed into robotic voice? What would robotic vocal virtuosity sound like? On a more philosophical level, as Dr Ron Chrisley has explored, &lt;a href=&quot;https://paics.wordpress.com/2018/05/14/robot-opera-robert-gyorgyi-interviews-ron-chrisley/&quot;&gt;what does it mean for a robot to sing&lt;/a&gt;, or alternatively - who is singing, when a robot sings?&lt;/p&gt;

&lt;h2 id=&quot;2017-robot-opera-a-mini-symposium&quot;&gt;2017 Robot Opera: a Mini Symposium.&lt;/h2&gt;

&lt;p&gt;Using commercially available robots already deployed in the School of Engineering and Informatics at Sussex, we launched the robot opera project with a half day event in June 2017. This consisted in a set of papers delivered by Sussex researchers (Ron Chrisley, Thanos Polymeneas-Liontiris, and Chris Kiefer) followed by performances of two five-minute operas, both directed by Tim Hopkins, &lt;em&gt;The Opposite of Familiarity&lt;/em&gt; composed by Ed Hughes with a libretto by Eleanor Knight,    and &lt;em&gt;O, One&lt;/em&gt; by Evelyn Ficarra, for two Nao robots. The robots were programmed by Ron Chrisley and the human musicians were    Alice Eldridge (cello) Joe Watson (piano). The talks ranged over subjects such as Learning and Performing with AIs (Kiefer); Operatic Bots (Polymeneas-liontiris); and What would it be for a robot to sing? (Chrisley), introduced by Evelyn Ficarra. Full documentation of the event, including talks and performances, can be seen &lt;a href=&quot;https://youtu.be/bV-seLR0OP0&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our creative work with the Nao robots (two toddler-sized robots designed and built by Softbank Robotics) revealed some interesting challenges. Our aim was that the primary ‘on stage’ performers would be the two robots, with the human musicians entirely in support in the background. We were determined the the Naos would sing with their own voices - that is, we would not simply record humans and put the voices inside the robots. We would create the ‘singing’ with the same voice the robots used to speak - accessed through the text to speech software that comes with the robot operating &amp;amp; animation system. Ron Chrisley and the composers met weekly to try out different techniques, with Ron programming in Python to effect the pitch and rhythmic pace of the robots. One interesting hitch was that the robots could not elongate a vowel sound, they could only repeat - so instead of ‘aaaaaah’ stretched out, they would sing a, a, a, a, a, - pronouncing each letter separately.&lt;/p&gt;

&lt;p&gt;Although the Naos do have some speech recognition and facial recognition, and we wanted to give a sense of them acting and relating to each other, in reality they were more like puppets, enacting pre-programmed sequences triggered by either a tap on their heads (where they have a button) or by showing them a particular symbol trigger which they are programmed to recognise to initiate a sequence. Tim Hopkins’s direction set the robots out on a sort of runway between two strips of seated audience members, with musicians at either end, so that the audience were effectively looking at each other over the heads of the performing robots. There were also robots in the audience, as Sussex has a fleet of Naos and we had asked all the Nao caretakers on campus to bring their robots with them, further blurring identities between robots and humans as both audience and performers. The ’set’ was a simple strip of white paper on the floor, marking out the performance space, on which ‘cave’ drawings showing the evolution of machines (an imagined past for the robots) were drawn, and on which we wrote out words of the libretto in real time during the performance. For further details on the music of &lt;em&gt;O, One&lt;/em&gt; by Evelyn Ficarra, see https://evelynficarra.net/portfolio/o-one/&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/16h2hOxxz5k&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;2019-robot-opera---whats-next&quot;&gt;2019 Robot Opera - What’s Next?&lt;/h2&gt;
&lt;p&gt;In 2019, we developed the project to focus on human robot interaction, particularly on how robotic vocal virtuosity might inform human vocal practice. This time we used a Pepper robot (Softbank Robotics) and brought in a human opera singer, Loré Lixenberg of Apartment House, together with her collaborator, cellist Anton Lukoszevieze, and a robot cat, because - of course you need a robot cat. Evelyn worked with two programmers, Deepeka Khosla and Kopiga Kugananthaval, to animate the robot and again explored how to push the robot’s ‘natural’ speaking voice into operatic and virtuosic expressive shapes. We commissioned Sam Bilbow to make a ‘robot cello’, so that both our human performers, Anton and Loré, had robotic counterpoints. The performance was co-devised by the creative team and again the event concluded with a reflective panel, this time chaired by Nick Till, with Ron Chrisley as first respondent and a Q &amp;amp; A featuring Evelyn and the performers. 
Taster clip:&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/394115792?h=17b451e420&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&quot;https://vimeo.com/394115792&quot;&gt;Robot Opera 2019 &amp;quot;I Want Your Job&amp;quot; excerpt&lt;/a&gt; from &lt;a href=&quot;https://vimeo.com/user87904852&quot;&gt;Evelyn Ficarra&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;2021-robo_po--robo_op&quot;&gt;2021 Robo_Po /// Robo_Op&lt;/h2&gt;

&lt;p&gt;This was a two day event consisting of a robot poetry reading and an operatic performance featuring Cleo, a robot designed and built by Engineered Arts. Both days were streamed live to an international audience and concluded with a reflective panel discussion by artists, academics and industry professionals. Cleo, unlike the Nao robots or Pepper, is a hyper-realistic humanoid robot, with silicon skin and real human hair. Cleo is full size, taller than many humans, and has face recognition and some limited AI driven features such as the ability to mimic certain movements. She has cameras in her eyes, and you can tap into the video stream of what she is seeing and project it. You can also ventriloquize her voice using a microphone - though we did not do that directly in real time.&lt;/p&gt;

&lt;p&gt;Cleo’s hyperrealistic appearance brought us into different dramatic and musical territory with this project, raising questions of the uncanny. This allowed us to explore ideas around human / robot mirroring and hybridity, both visually in the staging, and vocally in the music. Evelyn considered how human voices are made from our physical materiality of muscle and bone, and wanted to reflect the robot’s physicality in its voice. Therefore in this project she did not limit herself to using the robot’s built in text to speech mechanism. Instead, she visited the robot factory in Falmouth and recorded sounds of robot body movement and robot manufacture. She also met with Loré Lixenberg the opera singer to record her voice, and then chopped up and fragmented both machine and human recordings to create a hybrid vocal world for the robot, eventually also including its own speaking voice in heightened ways. The robot sounds also formed the basis of an electronic soundscape which supported the voices.&lt;/p&gt;

&lt;p&gt;We were very pleased in this new phase of the project to bring in a choreographer, Janine Fletcher, from Southeast Dance. Having learned through working with the Pepper robot about the importance of robotic movement in making the robot voice believable, we knew we needed an expert in movement this time, who could also work with the human performers. Janine Fletcher added a valuable dimension to the performance particularly in her duet with the robot, and spoke very interestingly afterwards about the complexities of performing with a body that has no awareness of others in the space - Cleo can see faces but does not have sensors on her body to prevent collisions, for example.&lt;/p&gt;

&lt;p&gt;See a short documentary on the Robo_Op project&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Tzzmp-9_Rpo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;:&lt;/p&gt;

&lt;p&gt;Full documentation of the performance and panel discussion:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/ZTiguwX6mtA&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;A panel discussion on Robo_Op:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/bWK86rzW7og?start=139&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;You can also see the robot poetry reading which occurred on day one of    Robo_Po /// Robo_Op here:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/qdB8jWdkX2w?start=1136&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For further scholarly reflection and context, see this excellent article by Alexander Sigman:
Sigman, A. (2020). Robot Opera: Bridging the Anthropocentric and the Mechanized Eccentric. Computer Music Journal, 43(1), 21–37. https://doi.org/10.1162/comj_a_00498&lt;/p&gt;

&lt;p&gt;For further information about robot opera at Sussex contact &lt;a href=&quot;https://profiles.sussex.ac.uk/p41192-evelyn-ficarra&quot;&gt;Evelyn Ficarra&lt;/a&gt; or &lt;a href=&quot;https://profiles.sussex.ac.uk/p208667-chris-kiefer&quot;&gt;Chris Kiefer&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate>
        <link>http://www.emutelab.org/blog/robot_opera</link>
        <guid isPermaLink="true">http://www.emutelab.org/blog/robot_opera</guid>
        
        <category>emutelab</category>
        
        <category>robot</category>
        
        <category>opera</category>
        
        <category>singing</category>
        
        <category>synthesis</category>
        
        <category>musical theatre</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>A Chat with Ingi Garðar Erlendsson</title>
        <description>&lt;h4 id=&quot;-a-chat-with-ingi-garðar-erlendsson-&quot;&gt;:::: A Chat with Ingi Garðar Erlendsson ::::&lt;/h4&gt;

&lt;p&gt;As a part of the work for the &lt;a href=&quot;https://feedback-musicianship.pubpub.org/&quot;&gt;AHRC Feedback Musicianship Network&lt;/a&gt;, Emute Lab Member Halldór Úlfarsson (IS) Interviews Composer and Thranophone player Ingi Garðar Erlendsson (IS) on his performance practice with Thranophone (developed together with composer Þráinn Hjálmarsson (IS)). The version of Thranophone Ingi Garðar has been playing for the past decade is a Tuba induced to feedback via speaker cone sitting in the bell, outputting the sound of a microphone Ingi keeps in his mouth (sheathed in a rubber membrane). His favoured controller for this setup is gain control for the amplification via foot pedal (with various effects injected in the signal path according to his fancy for a given performance). Here is Ingi Garðar giving us an insight into his feelings about playing thranophone in (January, 14. 2021):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In the beginning it was very raw but I liked the idea and then I kept trying things to make it more interesting. So I needed to find ways to make it more interesting as a sound source, for the tuba-thranophone it is a lot about how I use my mouth. But it really is a monster now because you have so many options for playing. Which I don’t think people recognized in the beginning, because it’s basically just feedback so I don’t think people recognized how rich it can be. The perception was that it is too primitive. But taking it from there if you, hmm, take care, if you practice the system, the instrument, if you develop a performance practice and work through [technical] problems that come up it’s cool. Being a composer it’s amazing that I cannot trust the instrument, that it is the same today as it was yesterday, that it is the same in one space or another. Although usually it’s quite similar.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/img/thranophoneSkull.png&quot; alt=&quot;Thranophone&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here’s Ingi playing at ICLI 2016:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/l5vDKEZsJjY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

</description>
        <pubDate>Sat, 10 Apr 2021 00:00:00 +0100</pubDate>
        <link>http://www.emutelab.org/blog/IngiInterview</link>
        <guid isPermaLink="true">http://www.emutelab.org/blog/IngiInterview</guid>
        
        <category>emutelab</category>
        
        <category>nime</category>
        
        <category>lab report</category>
        
        <category>feedback</category>
        
        <category>paper</category>
        
        <category>poster</category>
        
        <category>workshop</category>
        
        <category>performance</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Emute Lab at NIME 2020</title>
        <description>&lt;p&gt;&lt;strong&gt;:::: Emute Lab at NIME 2020 ::::&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Emute Lab members have a number of contributions to New Interfaces for Musical Expression 2020, taking place online in late July.  NIME is an annual conference on musical expression and the design of new musical instruments. It’s the natural home of many of our research projects, which is why we begin with a lab report on our activities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Instrumental Investigations at the Emute Lab&lt;/em&gt;&lt;/strong&gt;
&lt;em&gt;Thor Magnusson, Alice Eldridge and Chris Kiefer&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This lab report discusses recent projects and activities of the Experimental Music Technologies Lab at the University of Sussex. The lab was founded in 2014 and has contributed to the development of the field of new musical technologies. The report introduces the lab’s agenda, gives examples of its activities through common themes and gives short description of lab members’ work. The lab environment, funding income and future vision are also presented.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The remaining contributions centre around a big research topic in Emute Lab, feedback instruments and musicianship.  Halldór Úlfarsson collaborates on two submissions, firstly a paper:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Sculpting the behaviour of the Feedback-Actuated
Augmented Bass&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Halldór Úlfarsson and Adam Pultz Melbye (Sonic Arts Research Centre, Belfast)&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This paper describes physical and digital design strategies for the Feedback-Actuated Augmented Bass – a self-contained feedback double bass with embedded DSP capabilities. A primary goal of the research project is to create an instrument that responds well to the use of extended playing techniques and can manifest complex harmonic spectra while retaining the feel and sonic fingerprint of an acoustic double bass. While the physical configuration of the instrument builds on similar feedback string instruments being developed in recent years, this project focuses on modifying the feedback behaviour through low-level audio feature extractions coupled to computationally lightweight filtering and amplitude management algorithms. We discuss these adaptive and time-variant processing strategies and how we apply them in sculpting the system’s dynamic and complex behaviour to our liking.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/376697035&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&quot;https://vimeo.com/376697035&quot;&gt;The Augmented Double Bass (listen with headphones)&lt;/a&gt; from &lt;a href=&quot;https://vimeo.com/user27144102&quot;&gt;Adam Pultz Melbye&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Halldór also collaborates on a performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Dual/Duel/Duet/for/with/halldorophone&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Nicole Robson (Queen Mary, University of London) and Halldór Úlfarsson&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The halldorophone is a cello-like, feedback instrument, developed over the past decade by Halldór Úlfarsson. The
instrument is well-established in experimental music circles and gaining wider recognition thanks to its use by composer
and cellist Hildur Guðnadóttir in film scores, including her Oscar nominated music for Joker (2019). The halldorophone
utilises a simple system, whereby the vibration of each string is detected by a pickup, amplified and routed to a speaker
embedded in the back of the instrument. By adding gain to individual strings in the feedback loop, the instrument’s
response can become rapidly complex, potentially spinning out of control. While every musical performance of a
piece is unique in some way and contingent on its particular moment and situation in time, the unstable nature of the
halldorophone exacerbates this condition. Players describe the halldorophone as ‘unpredictable’, ‘very much alive’ and
as having ‘its own ideas’, even tiny changes to their body position in performance might produce unexpected effects
. In this NIME premiere for the instrument, cellist Nicole Robson will perform a piece for a new digitally endowed
halldorophone, and the title of the piece – Dual/Duel/Duet – acknowledges the active role of the instrument in shaping
the composition and performance.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://www.halldorophone.info/public/img/2018-Square.jpg&quot; alt=&quot;Sister 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next up is a paper on complexity and the behaviour of feedback instruments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Shaping the behaviour of feedback instruments with complexity-controlled gain dynamics&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Chris Kiefer, Dan Overholt (Aalborg University, Copenhagen), Alice Eldridge&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Feedback instruments offer radical new ways of engaging
with instrument design and musicianship. They are defined
by recurrent circulation of signals through the instrument,
which give the instrument ‘a life of its own’ and a ’stimulating uncontrollability’. Arguably, the most interesting
musical behaviour in these instruments happens when their
dynamic complexity is maximised, without falling into saturating feedback. It is often challenging to keep the instrument in this zone; this research looks at algorithmic ways
to manage the behaviour of feedback loops in order to make
feedback instruments more playable and musical; to expand
and maintain the ‘sweet spot’. We propose a solution that
manages gain dynamics based on measurement of complexity, using a realtime implementation of the Effort to Com-
press algorithm. The system was evaluated with four musicians, each of whom have different variations of string-based
feedback instruments, following an autobiographical design
approach. Qualitative feedback was gathered, showing that
the system was successful in modifying the behaviour of
these instruments to allow easier access to edge transition
zones, sometimes at the expense of losing some of the more
compelling dynamics of the instruments. The basic efficacy
of the system is evidenced by descriptive audio analysis.
This paper is accompanied by a dataset of sounds collected
during the study, and the open source software that was
written to support the research.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finally, we’ll be running a &lt;strong&gt;&lt;em&gt;workshop on Feedback Musicianship&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Workshop Aim: Exchange and generation of strategies, concepts and practices of feedback musicianship – building community, new musical collaborations
Feedback purposefully utilised in performance has long been an interesting musical endeavour; however, integrating such expertise into the design and development of instruments and interactive systems, which balance autonomy and expressivity, playability and musicality remains a challenge. Examples include extended traditional instruments, modular synthesisers, feedback incorporated into acoustic resonating bodies, new algorithmic techniques for managing feedback loops, etc. The workshop will conclude with an evening concert (online informal jam session) in the form of improvisation with feedback instruments, open to all participants. Identifying the importance of musical feedback in interaction, instruments, and systems, this workshop focuses on the development of instruments for innovative interactions with feedback in music, from designs for feedback instruments themselves, to novel multi-sensory interaction with feedback incorporated into augmented instruments and systems.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can see more details and sign up here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://nime2020.bcu.ac.uk/feedback-musicianship/&quot;&gt;https://nime2020.bcu.ac.uk/feedback-musicianship/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 26 Jun 2020 00:00:00 +0100</pubDate>
        <link>http://www.emutelab.org/blog/emute-at-nime</link>
        <guid isPermaLink="true">http://www.emutelab.org/blog/emute-at-nime</guid>
        
        <category>emutelab</category>
        
        <category>nime</category>
        
        <category>lab report</category>
        
        <category>feedback</category>
        
        <category>paper</category>
        
        <category>poster</category>
        
        <category>workshop</category>
        
        <category>performance</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Sema: Live Coding With Machine Learning Workshop</title>
        <description>&lt;p&gt;&lt;strong&gt;:::: When: June 29th - July 3rd, 3-5pm UK time. Where: Zoom ::::&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Would you like to use &lt;b&gt;machine learning as part of musical live coding&lt;/b&gt;? Would you like to create your own live coding language? We are inviting you to participate in a free workshop that will take place online in early July. With daily videos, Zoom sessions and follow-up online hangouts, we will get you up and running in using our new technologies for using AI in live coding.&lt;/p&gt;

&lt;p&gt;As part of our work in the &lt;a href=&quot;http://www.mimicproject.com&quot;&gt;MIMIC project&lt;/a&gt;, we have created &lt;b&gt;Sema&lt;/b&gt;: an online system for live coding with AI in the browser. Here you can apply many of the machine learning technologies we have implemented as part of our MIMIC work, but moreover: you can design your own live coding language!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/Semascreenshot.png&quot; alt=&quot;Screenshot of Sema&quot; /&gt;
&lt;em&gt;A screenshot of Sema. Take a look at the three videos of Sema in action below.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We believe every new live coding language results in new musical approaches, which implies that for a diversity of music we need a diversity of languages. Sema enables you to write your own language, whether it is one piece of music, an instrument, a pattern generator, or a full blown live coding language.&lt;/p&gt;

&lt;p&gt;Sema will be officially launched just before the start of the workshop, so you will be one of the first users to test this new technology and design your own live coding languages for machine learning in music.&lt;/p&gt;

&lt;p&gt;There is no previous experience required to participate in this Sema workshop. Although you do not need any machine learning expertise, you will benefit from beginner-level JavaScript programming skills. You don’t even have to be an experienced musician. This workshop will introduce the basic concepts of musical live coding with AI, and get people up to speed in using Sema and creating their own live coding languages. We are hoping that workshop participants will contribute in a user-study that will help us to develop the system further.&lt;/p&gt;

&lt;p&gt;We will run a flipped-learning workshop where we release introductory tutorial videos each day and workshop participants study them in their own time. We then have a synchronous Zoom Q&amp;amp;A workshop session at 3pm every day.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Programme of the Workshop Week:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monday - June 29th&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;all day - Introduction (YouTube videos): Introduction to Sema and the default Sema language
3pm - Workshop (Zoom): Making music with the Sema default language + QnA
5pm - Slack channel: Discussion
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Tuesday - June 30th&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;all day - Introduction (YouTube Videos): Machine Learning concepts. Using machine learning in Sema
1pm - Slack channel: Discussion and work
3pm - Workshop (Zoom): Making music with machine learning. Training a ML network + QnA
4pm - Slack channel: Discussion
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Wednesday - July 1th&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;all day - Introduction (YouTube Videos): Machine learning libraries. FFT and machine learning
1pm - Slack channel: Discussion and work
3pm - Workshop (Zoom): Machine listening and machine learning
4pm - Slack channel: Discussion
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Thursday - July 2nd&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;all day - Introduction (YouTube Videos): Language design in Sema. How to create your own live coding language
1pm - Slack channel: Discussion and work
3pm - Workshop (Zoom): Making your own piece with unique language syntax
5pm - Slack channel: Discussion
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Friday - July 3rd&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;all day - Introduction (YouTube Videos): Language design for machine learning	
1pm - Slack channel: Discussion and work
3pm - Workshop (Zoom): Finishing your live coding language + QnA
5pm - Showcase of projects in development (or ideas). Drinks and snack.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Additional Support the Following Week:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monday - July 6th&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;5pm - Project Development (Zoom): A session where we support people completing their projects (&amp;lt;b&amp;gt;note the change to 5pm&amp;lt;/b&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Wednesday - July 8th&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3pm - Project Development (Zoom): A session where we support people completing their projects
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Friday - July 10th&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3pm - Project Development (Zoom): A session where we support people completing their projects
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Registration:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;A sign-up and further information about the workshop: &lt;a href=&quot;https://bit.ly/30OomUo&quot;&gt;Sema Workshop Registration&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;This video demonstrates the &lt;b&gt;default language&lt;/b&gt; of Sema and some of its functionality&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/7Cu2R66OTak&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In this video we have a new language, called &lt;b&gt;Rubber Duckling&lt;/b&gt;, and we demonstrate some basic rhythm functionality.&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Qw4sYnTj-Ow&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This video shows the binary language called &lt;b&gt;Nibble&lt;/b&gt;. It functions by swapping bits around.&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/6wIgZ-Vymas&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;These videos are small examples of what can be done with Sema. It is really up to you to decide what you will do with this system.&lt;/p&gt;

&lt;p&gt;For a more technical information about Sema, please refer to these articles:&lt;/p&gt;

&lt;p&gt;Bernardo, F., Kiefer, C., Magnusson, T. (2019). An AudioWorklet-based Signal Engine for a Live Coding Language Ecosystem. In Proceedings of Web Audio Conference 2019, Norwegian University of Science and Technology (NTNU), Trondheim, Norway (Best Paper Award at Web Audio Conference 2019)
&lt;a href=&quot;https://webaudioconf.com/_data/papers/pdf/2019/2019_40.pdf&quot;&gt;https://webaudioconf.com/_data/papers/pdf/2019/2019_40.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;C. Kiefer and T. Magnusson. Live Coding Machine Learning and Machine Listening: A Survey on the Design of Languages and Environments for Live Coding. In Proc. of the International Conference on Live Coding., Madrid, 2019.
&lt;a href=&quot;https://iclc.toplap.org/2019/papers/paper97.pdf&quot;&gt;https://iclc.toplap.org/2019/papers/paper97.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Bernardo, F., Kiefer, C., Magnusson, T. (2020). Designing for a Pluralist and User-Friendly Live Code Language Ecosystem with Sema. 5th International Conference on Live Coding, University of Limerick, Limerick, Ireland&lt;/p&gt;

&lt;p&gt;Bernardo, F., Kiefer, C., Magnusson, T. (forthcoming). A Signal Engine for a Live Code Language Ecosystem. Journal of Audio Engineering Society, Vol. 68, No. 1, October&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Please register above and we hope to see you on Zoom, from June 29th!&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;(Francisco, Chris and Thor)&lt;/p&gt;

</description>
        <pubDate>Tue, 16 Jun 2020 00:00:00 +0100</pubDate>
        <link>http://www.emutelab.org/blog/Semaworkshop</link>
        <guid isPermaLink="true">http://www.emutelab.org/blog/Semaworkshop</guid>
        
        <category>emutelab</category>
        
        <category>workhop</category>
        
        <category>live coding</category>
        
        <category>machine learning</category>
        
        <category>language design</category>
        
        <category>summer</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Performance Lecture: David Rothenberg</title>
        <description>&lt;p&gt;&lt;b&gt;:::: Thursday, May 7th, 4.00pm ONLINE ::::&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Performance Lecture: &lt;b&gt;The Virtual Nightingale&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Have you found that you have paid more attention to the birds and the bees during lock down?&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;To segue from International Dawn Chorus Day and continue the celebration of our soniferous cousins, we are thrilled to host a special guest from across the pond.&lt;/p&gt;

&lt;p&gt;David Rothenberg, author of &lt;strong&gt;Why Birds Sing&lt;/strong&gt; and &lt;strong&gt;Nightingales in Berlin&lt;/strong&gt;, veteran performer with nature sounds near and far,
will discuss his work with nightingales and underwater pond insects, explaining why human music can be enhanced by taking
the sounds of the natural world seriously.&lt;/p&gt;

&lt;p&gt;This event is free and open to all.  See bottom for registration details.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://scontent-lhr8-1.xx.fbcdn.net/v/t1.0-9/95500812_644695099715434_6391060066870493184_o.jpg?_nc_cat=111&amp;amp;_nc_sid=b386c4&amp;amp;_nc_ohc=qQ8Upx15yEsAX-77h8n&amp;amp;_nc_ht=scontent-lhr8-1.xx&amp;amp;oh=bb4c10ed93adb42a950191cd1d937782&amp;amp;oe=5ED6EC87&quot; alt=&quot;rothenberg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dr. David Rothenberg is distinguished professor of philosophy and music at the New Jersey Institute of Technology.  He is a noted writer on themes connecting humanity, nature, and technology and music.  He is the author of Why Birds Sing (Basic Books and Penguin UK, 2005), also published in Italy, Spain, Taiwan, China, Korea, and Germany, and turned into a feature-length documentary &lt;em&gt;Why Birds Sing&lt;/em&gt; by Endemol UK for BBC4 in June, 2007.  Rothenberg is also the author of &lt;em&gt;Hand’s End: Technology and the Limits of Nature&lt;/em&gt; (California, 1993), &lt;em&gt;Thousand Mile Song&lt;/em&gt; (Basic Books, 2008),  &lt;em&gt;Survival of the Beautiful&lt;/em&gt; (Bloomsbury, 2011), Bug Music (St Martins, 2013) and &lt;em&gt;Nightingales in Berlin&lt;/em&gt; (Chicago, 2019 and Rowohlt, 2020)&lt;/p&gt;

&lt;p&gt;As a musician Rothenberg records for the prestigious ECM label.  His CD &lt;em&gt;One Dark Night I Left My Silent House&lt;/em&gt;,  a duet album with pianist Marilyn Crispell, was called “une petite miracle” by Le Monde and named by The Village Voice one of the ten best CDs of 2010.  He has performed or recorded with Peter Gabriel, Scanner, Suzanne Vega, Marilyn Crispell, Pete Seeger, and Jaron Lanier, among many others, and appears on numerous CDs playing clarinets and various electronic and natural sounds.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/3ZosukkjTjk&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=3ZosukkjTjk&quot;&gt;New York-Paris-Berlin Live Nightingale Concert performed virtually under quarantine.&lt;/a&gt; More videos  &lt;a href=&quot;https://www.youtube.com/user/whybirdssing/videos&quot;&gt; here &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This event is free and open to all.  &lt;a href=&quot;https://universityofsussex.zoom.us/meeting/register/tJIpd-qqpjgoGten8-CZjawu5JEKJNl4eG5c&quot;&gt;Register in advance for this meeting.&lt;/a&gt;
After registering, you will receive a confirmation email containing information about joining the meeting.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;www.davidrothenberg.net&quot;&gt;www.davidrothenberg.net&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;www.nightingalesinberlin.com&quot;&gt;www.nightingalesinberlin.com&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.youtube.com/user/whybirdssing/videos&quot;&gt;www.youtube.com/user/whybirdssing/&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;video-responsive&quot;&gt;
    &lt;iframe width=&quot;840&quot; height=&quot;3000&quot; src=&quot;https://universityofsussex.zoom.us/rec/share/pv53HZzXqTJLE4nGt27_fIwFBdi9aaa81HRI86IJyE4qYivLlLMKdV2QchEvqwDZ?startTime=1588863984000&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
        <pubDate>Fri, 01 May 2020 00:00:00 +0100</pubDate>
        <link>http://www.emutelab.org/blog/Rothenberg</link>
        <guid isPermaLink="true">http://www.emutelab.org/blog/Rothenberg</guid>
        
        <category>emutelab</category>
        
        <category>event</category>
        
        <category>practice</category>
        
        <category>philosophy</category>
        
        <category>ecoacoustics</category>
        
        <category>improvisation</category>
        
        <category>multi-species music</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Materialities symposium</title>
        <description>&lt;p&gt;&lt;img src=&quot;/img/emutemat.jpg&quot; alt=&quot;emutemat&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Materialities symposium&lt;/h1&gt;

&lt;p&gt;Join us on the morning of Thursday 5th of December, 2019 for a fast-paced symposium on the subject of materialities in music making.&lt;/p&gt;

&lt;p&gt;We have two great workshops/presentations followed by a panel discussion around notions of the influence of materiality on music and sound making.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Workshops/Presentations&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://trsound.bandcamp.com/&quot;&gt;Tom Richards&lt;/a&gt; will present a workshop which invites participants to utilise his Voltage Controlled Turntables with some modular synthesiser equipment. Imagine scratching a record with LFO control, or changing pitch. 
Participants are encouraged to bring vinyl records to this session.&lt;/p&gt;

&lt;p&gt;Tom Richards is a musician, sound designer, artist, researcher and instrument maker, working in London UK. He has performed and exhibited widely in the UK, as well as internationally in the US, Germany, Peru, Singapore, Hungary, Japan and Sweden. Selected works and live performances have taken place at Tate Britain, The Queen Elizabeth Hall, The Science Museum, Spike Island, Cafe Oto, MK Gallery, Focal Point, and Camden Arts Centre. He has recently finished his PhD (Goldsmiths/Science Museum) on the work of Daphne Oram: electronic music pioneer, and founder member of the BBC Radiophonic Workshop. This research included the construction of Oram’s Mini Oramics synthesizer design (Circa 1975), a project that has since gained worldwide attention. He is represented by Nonclassical.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;a href=&quot;http://grrrr.org/&quot;&gt;Thomas Grill&lt;/a&gt;&lt;/b&gt; and &lt;b&gt;&lt;a href=&quot;http://tai-studio.org/&quot;&gt;Till Bovermann&lt;/a&gt;&lt;/b&gt; present activities relating to the &lt;b&gt;&lt;a href=&quot;https://rottingsounds.org/&quot;&gt;Rotting Sounds project&lt;/a&gt;&lt;/b&gt;. Here degradation in digital systems will be explored and discussed. 
Thomas Grill (University of Music and Performing Arts Vienna) is the project manager and principal investigator. He has ample experience in both scientific and artistic research and has been composing, performing and exhibiting with digital sound for over 20 years.
Till Bovermann (University of Applied Arts Vienna) is his main discourse partner. In his artistic works, Till addresses the relationship between seemingly contradictory elements, e.g., the digital and physical realm.
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Round table/panel discussion - notions of materiality and influence on practice&lt;/b&gt;
As the final event in the symposium, topics relating to notions of materiality and influence on practice are discussed by a panel of academics/composers and performers. This promises to be a lively debate relating to media archeology, liveness and audience perspective.&lt;/p&gt;

&lt;p&gt;Evelyn Ficarra, Senior Lecturer in Music  &lt;a href=&quot;http://www.sussex.ac.uk/profiles/41192&quot;&gt;http://www.sussex.ac.uk/profiles/41192&lt;/a&gt;&lt;br /&gt;
Tom Richards&lt;br /&gt;
Till Bovermann&lt;br /&gt;
Thomas Grill&lt;br /&gt;
Further panel members and chair (TBC)&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.eventbrite.co.uk/e/materialities-emute-lab-music-symposium-tickets-82606421029&quot;&gt;Event registration page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;NOTE:
This event follows the live EMUTE LAB 5: Materialities gig at The Rose Hill on the 4th of December. Details here: 
http://www.emutelab.org/blog/emutelab5
https://www.facebook.com/events/720543615076362&lt;/p&gt;

&lt;p&gt;See also:
&lt;a href=&quot;http://www.emutelab.org/blog/emutelab5&quot;&gt;http://www.emutelab.org/blog/emutelab5&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;The Rose Hill 4th December 2019, doors 7:30&lt;/b&gt;
£6 / £4&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Advance tickets:&lt;/b&gt; &lt;a href=&quot;https://www.eventbrite.co.uk/e/emute-labs-5-materialities-tickets-80358270751?fbclid=IwAR24wQuZiU43zIYNhlPd3hE5FU0fPazCZfJV3cSZxK5kPK-2F-UcS1pKh2I&quot;&gt;https://www.eventbrite.co.uk/e/emute-labs-5-materialities-tickets-80358270751?fbclid=IwAR24wQuZiU43zIYNhlPd3hE5FU0fPazCZfJV3cSZxK5kPK-2F-UcS1pKh2I&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;@emutelab&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.therosehill.co.uk&quot;&gt;https://www.therosehill.co.uk&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.facebook.com/events/720543615076362&quot;&gt;https://www.facebook.com/events/720543615076362&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate>
        <link>http://www.emutelab.org/blog/materialities</link>
        <guid isPermaLink="true">http://www.emutelab.org/blog/materialities</guid>
        
        <category>emutelab</category>
        
        <category>event</category>
        
        <category>experimental</category>
        
        <category>materialities</category>
        
        <category>lathe cutting</category>
        
        <category>vinyl</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Emute Lab 5 @ The Rose Hill</title>
        <description>&lt;p&gt;&lt;img src=&quot;/img/emutelab5.jpg&quot; alt=&quot;EmuteLab5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Emute Lab presents: Materialities (04/12/19)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;An evening of experimental music celebrating materialities in music making. From deceased instruments manipulated as digital fragments to turntables controlled by synthesisers, live record cutting and electronics.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Artists:&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Rotting Sounds&lt;/b&gt; &lt;a href=&quot;http://rottingsounds.org&quot;&gt;http://rottingsounds.org&lt;/a&gt;
&lt;br /&gt;Two solo performances joined under the Rotting Sounds project and by a common theme.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/AnnTJvAejzo&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;b&gt;Thomas Grill: Musical material&lt;/b&gt;
&lt;br /&gt;Probing of fragments of deceased instruments by use of digital sound.Sounding the materials, shapes, resonances – tracing remnants of a musical life.
&lt;br /&gt;
&lt;a href=&quot;http://grrrr.org&quot;&gt;http://grrrr.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;AND&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Till Bovermann: Buffer manipulations&lt;/b&gt;&lt;br /&gt;
Probing and fragmentation of deceased digital sound.
Sounding the materials, shapes, resonances – tracing remnants of a brief ephemerality.&lt;br /&gt;
&lt;a href=&quot;http://lfsaw.de&quot;&gt;http://lfsaw.de&lt;/a&gt;
&lt;a href=&quot;https://lfsaw.bandcamp.com/album/re-interpretation&quot;&gt;https://lfsaw.bandcamp.com/album/re-interpretation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Tom Richards: Voltage controlled turntables and electronics&lt;/b&gt;&lt;br /&gt;
Performing with modified turntables and synthesisers&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://trsound.bandcamp.com/&quot;&gt;https://trsound.bandcamp.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Tom Richards is a musician, sound designer, artist, researcher and instrument maker, working in London UK. He has been working between sonic art, sculpture, film and music since graduating with an MA in Fine Art from Chelsea College of Art in 2004. Richards has built his own idiosyncratic modular electronic music system, with which he creates slowly evolving and heavily textured polyrhythmic improvisations. He has performed and exhibited widely in the UK, as well as internationally in the US, Germany, Peru, Singapore, Hungary, Japan and Sweden.
Selected works and live performances have taken place at Tate Britain, The Queen Elizabeth Hall, The Science Museum, Spike Island, Cafe Oto, MK Gallery, Focal Point, and Camden Arts Centre. 
He has recently finished his PhD (Goldsmiths/Science Museum) on the work of Daphne Oram: electronic music pioneer, and founder member of the BBC Radiophonic Workshop. This research included the construction of Oram’s Mini &lt;a href=&quot;https://www.bbc.co.uk/news/technology-36651270&quot;&gt;Oramics synthesizer&lt;/a&gt; design (Circa 1975), a project that has since gained worldwide attention.
He is represented by &lt;a href=&quot;https://www.nonclassical.co.uk/music&quot;&gt;Nonclassical&lt;/a&gt;.&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/258282473?title=0&amp;amp;byline=0&amp;amp;portrait=0&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;
&lt;a href=&quot;https://vimeo.com/258282473&quot;&gt;Maximum Overdrive - Performance with Tom Richards&lt;/a&gt; from &lt;a href=&quot;https://vimeo.com/user40747262&quot;&gt;Tom Lock&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Furrowed: Live record cutting and modular synthesiser&lt;/b&gt;&lt;br /&gt;
New alias and project of Emute member Dylan Beattie. By using a record lathe and electronics a sound world is built around the material properties of live cut vinyl records.  &lt;br /&gt;
&lt;a href=&quot;http://furrowedsound.co.uk/&quot;&gt;http://furrowedsound.co.uk/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/db.jpg&quot; alt=&quot;Furrowedimage&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;The Rose Hill 4th December 2019, doors 7:30&lt;/b&gt;
£6 / £4&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Advance tickets:&lt;/b&gt; &lt;a href=&quot;https://www.eventbrite.co.uk/e/emute-labs-5-materialities-tickets-80358270751?fbclid=IwAR24wQuZiU43zIYNhlPd3hE5FU0fPazCZfJV3cSZxK5kPK-2F-UcS1pKh2I&quot;&gt;https://www.eventbrite.co.uk/e/emute-labs-5-materialities-tickets-80358270751?fbclid=IwAR24wQuZiU43zIYNhlPd3hE5FU0fPazCZfJV3cSZxK5kPK-2F-UcS1pKh2I&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;@emutelab&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.therosehill.co.uk&quot;&gt;https://www.therosehill.co.uk&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.facebook.com/events/720543615076362&quot;&gt;https://www.facebook.com/events/720543615076362&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate>
        <link>http://www.emutelab.org/blog/emutelab5</link>
        <guid isPermaLink="true">http://www.emutelab.org/blog/emutelab5</guid>
        
        <category>emutelab</category>
        
        <category>event</category>
        
        <category>experimental</category>
        
        <category>materialities</category>
        
        <category>lathe cutting</category>
        
        <category>vinyl</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>What's Happening in the World of Feedback Instruments?</title>
        <description>&lt;p&gt;At Emute Lab, several of our members have been concentrating their research efforts on building and performing with feedback instruments. You could define a Feedback Instrument as an instrument whose behaviour depends on the circulation of signals, and manipulation of the way in which these signals circulate around the instrument.  These instruments could be acoustic, electronic, digital, or hybrid.  Feedback will be the core of how they make sound and/or how musicians interact with them.&lt;/p&gt;

&lt;p&gt;There’s a great deal of activity in this area around the world at the moment, and here, I will attempt to highlight some of the recent developments in feedback instrument design and musicianship.  But before leaping into current work, let’s take a look at some past instruments and pieces that have had a big influence on today’s developments.&lt;/p&gt;

&lt;h4 id=&quot;eliane-radigue-usral&quot;&gt;Eliane Radigue: Usral&lt;/h4&gt;

&lt;p&gt;This is from Radigue’s collection of early feedback works.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/C_3Fu8YfSdI?start=684&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;max-neuhaus-fontana-mix-feed&quot;&gt;Max Neuhaus: &lt;a href=&quot;http://brainwashed.com/index.php?option=com_content&amp;amp;view=article&amp;amp;id=2399%3Amax-neuhaus-qfontana-mix-feed-six-realizations-of-john-cageq&amp;amp;catid=13%3Aalbums-and-singles&amp;amp;Itemid=1&quot;&gt;Fontana Mix-Feed&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;This piece uses contact microphone-speaker feedback through tympanis.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/x-cLk7TZuNY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;alvin-lucier-bird-and-person-dyning&quot;&gt;Alvin Lucier: &lt;a href=&quot;http://www.alvin-lucier-film.com/bird.html&quot;&gt;Bird and Person Dyning&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Speaker-binaural headphone feedback.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/6jLYof8sU4s&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;nicolas-collins-pea-soup&quot;&gt;Nicolas Collins: &lt;a href=&quot;https://www.nicolascollins.com/aboutpeasoup.htm&quot;&gt;Pea Soup&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;This piece uses shifting microphone-room feedback. Originally created in 1974, but here’s a newer revision from 2010.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/W7f5Iha7JyQ&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;david-tudor-neural-network-plus&quot;&gt;David Tudor: &lt;a href=&quot;http://www.lovely.com/albumnotes/notes1602.html&quot;&gt;Neural Network Plus&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Feedback was a key part of Tudors compositions and installations.  This piece from the early 90s was created using repurposed neuromorphic hardware, which enables the creation of highly complex analog feedback systems.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/OyzeqA9-DiI&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In Emute Lab, feedback instruments take a variety of forms.  Halldór Úlfarsson is the designer of the &lt;a href=&quot;https://www.halldorophone.info&quot;&gt;Halldorophone&lt;/a&gt;, and a growing family of new instruments that are descendents of his design. This family of string instruments use inbuilt speakers and exciters, together with individual string pickups and external processing to create a feedback loop. Halldór is researching his PhD in Emute Lab, and has recently developed two new instruments: The Sisters.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.halldorophone.info/public/img/2018-Square.jpg&quot; alt=&quot;Sister 1&quot; /&gt;
&lt;img src=&quot;https://www.halldorophone.info/public/img/2018-Standing.png&quot; alt=&quot;Sister 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is an early Halldorophone from 2009, played by Hildur Guðnadóttir.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/uo4Jq-_tysc&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Hildur plays the Halldorophone on several film soundtracks, including the Chernobyl soundtrack, for which she recently won an Emmy award.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/qcEPbx_iSjw&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This video shows a Halldorophone-DIMA-A duet. Composition by Johan Svensson, performed by My Hellgren and Jari Suominen&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/DKJvAIadFWE&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Alice Eldridge and I collaborated with Halldór to develop the &lt;a href=&quot;https://www.feedbackcell.info/&quot;&gt;Feedback Cello&lt;/a&gt;.  These instruments are based in the Halldorophone design, as augmentations (and/or invasions) of the traditional acoustic cello.  They have a speaker mounted in the back of the cello.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/feedback_cello_speaker.jpg&quot; alt=&quot;Feedback Cello Speaker&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We have built two of these instruments, with different augmentations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/feedback_cellos2.jpg&quot; alt=&quot;Feedback Cello Speaker&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here’s Alice playing at Fete Quaqua from 2016&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/180304450&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Here is a recording of a Feedback Cell performance from 2017, using the two cellos.&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/363528685&quot; width=&quot;640&quot; height=&quot;361&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Halldór is exploring augmentation of acoustic instruments further, developing a feedback double bass with &lt;a href=&quot;http://thanospl.net/?p=620&quot;&gt;Thanos Polymeneas-Liontiris&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/thanos_fbdbass.jpg&quot; alt=&quot;Feedback Double Bass&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above three instruments, together with &lt;a href=&quot;http://www.ixi-audio.net/&quot;&gt;Thor Magnusson&lt;/a&gt; on Threnoscope, form &lt;a href=&quot;https://www.emutelab.org/braindeadensemble/&quot;&gt;Brain Dead Ensemble&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/bdeRoseHill2.jpg&quot; alt=&quot;Brain Dead Ensemble, Live at The Rose Hill, Brighton&quot; /&gt;
You could call this a ‘feedback ensemble’, as the Threnoscope is connected to all of the instruments, and the instruments affect each other acoustically during performance. This is their album ‘EFZ’.&lt;/p&gt;

&lt;iframe src=&quot;https://open.spotify.com/embed/artist/54rJ3o7aXEg9bzQ3MukIYh&quot; width=&quot;300&quot; height=&quot;380&quot; frameborder=&quot;0&quot; allowtransparency=&quot;true&quot; allow=&quot;encrypted-media&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Halldór has also developed a &lt;a href=&quot;http://halldor.gr/spetson/2019/07/01/fbdb.html&quot;&gt;Feedback Double Bass with Adam Pultz&lt;/a&gt;, this time invasively with a mounted speaker.&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/345985776&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Emute Lab member Joe Watson explores nested feedback networks and modular synthesis, ‘The Thing Breathed’.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/H8LlbtgdB5M&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;You can read some more about this and brain dead ensemble in the &lt;a href=&quot;https://live-interfaces.github.io/2018/ICLI2018.pdf&quot;&gt;ICLI 2018 proceedings&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Moving outside of Emute Lab, here are some more instruments based on the feedback principle.&lt;/p&gt;

&lt;p&gt;Tom Davis and Laura Reid: The Feral Cello.  This instrument has an exciter and pickup, that runs through DSP processing in a duet between cellist and laptopist.&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/237465790&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This is Dan Overholt’s &lt;a href=&quot;https://vimeo.com/26661494?fbclid=IwAR0Qb5aAAhpIB9ZggqdsoVot8x6lcgg1qx-gkWYbbobGkdY_SfIyGdyBE4M&quot;&gt;Overtone Fiddle&lt;/a&gt;. Dan says: ‘This performance uses feedback via DSP filter-banks, set up to emulate an extra bank of resonant strings (simulating to some extent, the “hardanger fiddle” setup but with variable/extended feedback resonances).’&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/26766506&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This is the Feedback Lapsteel, by Jiffer Harriman.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/79R6oaVdmnk&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ijin.no/feedbacker.htm&quot;&gt;Christian Blandhoel&lt;/a&gt; is a Norwegian noise musician, who has created a expansive range of &lt;a href=&quot;https://drive.google.com/file/d/1b7nw3aEj_tvvPHM-qRB1R8qVTUiyPNyF/view&quot;&gt;Feedbackers&lt;/a&gt;.  Here’s a video of one of them, and more on his youtube channel.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/lHq_7MlIeFY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Luigi Marino: Cymbals with a handheld feedback device (containing a contact mic and exciter).&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/SaToWLbI-UA?start=962&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This video shows a concert with two feedback instruments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://tai-studio.org/portfolio/half-closed-loop.html&quot;&gt;Half-closed Loop&lt;/a&gt; by Till Boverman (from the beginning). This is custom-designed, using a brass pipe, wooden board with exciters and microphones.  In the second half (from 11:50) we move into feedback brass, with Ingi Garðar Erlendsson playing the &lt;a href=&quot;http://thrainnhjalmarsson.info/&quot;&gt;Thranaphone #2&lt;/a&gt;, a feedback tuba (with Eiríkur Orri Ólafsson playing Trumpet).  This instrument has a speaker mounted in the tuba and a mouthpiece microphone; the signal is processed through guitar pedals.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/l5vDKEZsJjY?start=0&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Jeff Snyder’s Feedback Trombone runs on a similar principle:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/yT6Vq9jGC7w&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href=&quot;http://lesleyflanigan.com/&quot;&gt;Lesley Flanigan&lt;/a&gt; uses custom-instruments based around microphone-speaker feedback.  This is a concert from the Guggenheim Museum, New York from 2014.&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/122308468&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;There are also some great software instruments out there.  Tom Mudd uses a network of interacting chaotic oscillators in Gutter Synthesis.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Y1qQd4DVIR4&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href=&quot;https://dariosanfilippo.tumblr.com/&quot;&gt;Dario Sanfilippo&lt;/a&gt; specialises in music generated from complex adaptive feedback systems; this is a performance of ‘Single-Fader Versitility’.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/hIrqmDif5uQ&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In audio-visual feedback systems, there’s a cross-modal flow between sound and image.   This is Mick Grierson with a live performance of ‘Stench’.&lt;/p&gt;

&lt;video muted=&quot;&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/img/StenchGig.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;These are some highlights of contemporary feedback instruments, which take a range of forms. These instruments can be challenging but also extremely rewarding to play. The challenges come from dealing with the complexity of highly sensitive dynamical systems, and changing from conventional modes of instrumental control to shared agency with the instrument.  The rewards are to break into new sonic territories and to discover some radically new ways of playing music.&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Oct 2019 00:00:00 +0100</pubDate>
        <link>http://www.emutelab.org/blog/feedback_insts</link>
        <guid isPermaLink="true">http://www.emutelab.org/blog/feedback_insts</guid>
        
        <category>emutelab</category>
        
        <category>feedback</category>
        
        <category>halldorophone</category>
        
        <category>cello</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Emute Lab 4 @ The Rose Hill</title>
        <description>&lt;p&gt;&lt;img src=&quot;/img/emutelab4.jpg&quot; alt=&quot;EMuTeLab2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Emute Lab presents: musically intelligent machines&lt;/p&gt;

&lt;p&gt;Emute Lab 4: Musically Intelligent Machines&lt;/p&gt;

&lt;p&gt;Come and hear new music created by musicians in collaboration with artificially intelligent machines.&lt;/p&gt;

&lt;p&gt;Expect some weird and wonderful experiments with new sounds and new musical instruments.&lt;/p&gt;

&lt;p&gt;Artists:&lt;/p&gt;

&lt;p&gt;MARIJE BAALMAN: gestural live coding&lt;/p&gt;

&lt;p&gt;MNISTREL: Live coding and uncanny interfaces&lt;/p&gt;

&lt;p&gt;SHELLY KNOTTS&lt;/p&gt;

&lt;p&gt;ANUZAK&lt;/p&gt;

&lt;p&gt;EVERYSONGIOWN: A Quantity Approach to Music Making&lt;/p&gt;

&lt;p&gt;and more artists who will be performing pieces made at our summer AI and music workshop&lt;/p&gt;

&lt;p&gt;The Rose Hill
£6 / £4
Advance tickets: &lt;a href=&quot;https://bit.ly/2JbzYXn&quot;&gt;https://bit.ly/2JbzYXn&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;@emutelab&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.mimicproject.com&quot;&gt;https://www.mimicproject.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.therosehill.co.uk&quot;&gt;https://www.therosehill.co.uk&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.facebook.com/events/936387710040807/&quot;&gt;https://www.facebook.com/events/936387710040807/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 26 Jun 2019 00:00:00 +0100</pubDate>
        <link>http://www.emutelab.org/blog/emutelab4</link>
        <guid isPermaLink="true">http://www.emutelab.org/blog/emutelab4</guid>
        
        <category>emutelab</category>
        
        <category>event</category>
        
        <category>improvisation</category>
        
        <category>audiovisual</category>
        
        <category>AI</category>
        
        <category>machine learning</category>
        
        <category>MIMIC</category>
        
        
        <category>blog</category>
        
      </item>
    
  </channel>
</rss>
