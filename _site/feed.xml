<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Emute Lab</title>
    <description>A Music Informatics and Performance Technologies Lab based in the School of Media, Film and Music at the University of Sussex</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 18 Mar 2024 15:08:10 +0000</pubDate>
    <lastBuildDate>Mon, 18 Mar 2024 15:08:10 +0000</lastBuildDate>
    <generator>Jekyll v4.3.2</generator>
    
      <item>
        <title>March Lab Meeting: Strange Attractors and Livecoding Modular Synths</title>
        <description>&lt;p&gt;On Tuesday 19th of March we have a double bill:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Max Worgan will be showing a couple of bits of musical software he has been developing around strange attractors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Chris Kiefer and Dimitris Kyriakoudis will be talking/demoing the uSEQ live coding module, some recent developments with the firmware &amp;amp; hardware, and the collaboration they are doing with Music Thing Modular to port the uSEQ firmware to their upcoming hybrid and programmable module + a new editor.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lnfiniteMonkeys/uSEQ&quot;&gt;https://github.com/lnfiniteMonkeys/uSEQ&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Date: Tuesday 19th of March
Time: 1pm
Location: Sussex Humanities Lab
Zoom: https://universityofsussex.zoom.us/j/99804772625&lt;/p&gt;

</description>
        <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/lab-meeting</link>
        <guid isPermaLink="true">http://localhost:4000/blog/lab-meeting</guid>
        
        <category>meeting</category>
        
        <category>AI</category>
        
        <category>chaos</category>
        
        <category>attractors</category>
        
        <category>modular</category>
        
        <category>livecoding</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Google Summer of Code 2024 Project: Differentiable Logic Gate Networks for Real-Time Interaction</title>
        <description>&lt;p&gt;&lt;img src=&quot;/img/gsoc2024.png&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Jack Armitage (from Intelligent Instruments Lab, Iceland) is partnering up with Chris Kiefer to explore real-time embedded AI again, only this time based on the work of Stanford’s Felix Peterson on Differentiable Logic Gate Networks.&lt;/p&gt;

&lt;p&gt;If you’re interesting in applying for this project, read the full project description over at the BeagleBoard forum, and get in touch!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://forum.beagleboard.org/t/embedded-differentiable-logic-gate-networks-for-real-time-interactive-and-creative-applications/37768&quot;&gt;https://forum.beagleboard.org/t/embedded-differentiable-logic-gate-networks-for-real-time-interactive-and-creative-applications/37768&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;More links:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://summerofcode.withgoogle.com/&quot;&gt;https://summerofcode.withgoogle.com/&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Timeline: &lt;a href=&quot;https://developers.google.com/open-source/gsoc/timeline&quot;&gt;https://developers.google.com/open-source/gsoc/timeline&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;BeagleBoard GSOC pages (including guides): &lt;a href=&quot;https://gsoc.beagleboard.org/&quot;&gt;https://gsoc.beagleboard.org/&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;BeagleBoard Discord (with #gsoc channel): &lt;a href=&quot;https://discord.gg/cNr5TSWZRz&quot;&gt;https://discord.gg/cNr5TSWZRz&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/difflog-gsoc</link>
        <guid isPermaLink="true">http://localhost:4000/blog/difflog-gsoc</guid>
        
        <category>meeting</category>
        
        <category>gsoc</category>
        
        <category>AI</category>
        
        <category>differentiable logic</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Lab Meeting with Victor Shepardson</title>
        <description>&lt;p&gt;We are holding an Emute Lab Monthly meeting, roughly on the final Tuesday of each month.&lt;/p&gt;

&lt;p&gt;We are really excited to be able to host Victor Shepardson this month.&lt;/p&gt;

&lt;p&gt;Victor is a post-graduate research at the Intelligent Instrument Lab in Reykjavik (http://www.https://iil.is) where he is exploring a lived in approach to designing with AI for new musical instruments.  Victor has kindly agreed to talk us through the art of training RAVE Models (Realtime Audio Variational autoEncoder models https://github.com/acids-ircam/RAVE).  From there we hope to expand the discussion to how we might manipulate the resulting model in different ways.&lt;/p&gt;

&lt;p&gt;Date: Tuesday 27th February
Time: 1pm
Location: Sussex Humanities Lab
Zoom: This is an open list so please email me for the Zoom link if you wish to join.&lt;/p&gt;

</description>
        <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/rave-lab-meeting</link>
        <guid isPermaLink="true">http://localhost:4000/blog/rave-lab-meeting</guid>
        
        <category>meeting</category>
        
        <category>AI</category>
        
        <category>rave</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Steve Symons wins Sussex AI Demo Award</title>
        <description>&lt;p&gt;Congratulations to Emute Lab PhD student Steve Symons, who won the £250 prize for best demo at the recent Sussex AI launch event.  Steve was showing his ‘entangled instrument’, which used a neural network to map a co-played instrument to different RAVE models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sussexAI1.jpeg&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/sussexAI2.jpeg&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/sussexAI3.jpeg&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.sussex.ac.uk/research/centres/ai-research-group/&quot;&gt;https://www.sussex.ac.uk/research/centres/ai-research-group/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://entangled-instruments.xyz&quot;&gt;http://entangled-instruments.xyz&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/demo-prize</link>
        <guid isPermaLink="true">http://localhost:4000/blog/demo-prize</guid>
        
        <category>sussex AI</category>
        
        <category>demo</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Emute Lab Research Fellow Post</title>
        <description>&lt;p&gt;There is a new Research Fellow post, working with Chris Kiefer on his new AHRC ‘Musically Embodied Machine Learning’ project (more info on this later…)&lt;/p&gt;

&lt;p&gt;The info for the post is here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://jobs.sussex.ac.uk/job/d9c1cb82-45cd-49f2-8642-0d8b49188e33&quot;&gt;https://jobs.sussex.ac.uk/job/d9c1cb82-45cd-49f2-8642-0d8b49188e33&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This position is for a post-doctoral researcher on the AHRC funded project Musically Embodied Machine Learning (MEML), within the Department of Music.  The project is an investigation into the musically expressive potential of machine learning when embodied within physical musical instruments.&lt;/p&gt;

&lt;p&gt;It proposes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tuneable ML&lt;/code&gt;, an approach to exploring the musicality of ML models, when they can be adjusted, personalised and remade, using a musical instrument as the interface. The project asks how instruments can be designed to make effective and musical use of embedded ML processes, and questions the implications for instrument designers and musicians when tunable ML processes are a fundamental driver of an instrument’s musical feel and musical behaviour.&lt;/p&gt;

&lt;p&gt;The role will encompass co-designing new musical instruments with practicing musicians, and evaluating the instruments through their experiences.   The project will offer opportunities for developing skills in creative artificial intelligence, musical instrument design (hardware and software) and participatory research methods.  There will be opportunities for publication, industry collaboration, conference and trade show attendance, and development and release of open source software and hardware.&lt;/p&gt;

&lt;p&gt;The post is for 18 months, 0.5fte, and the work will be conducted on campus in Brighton. The candidate will become a member of the Experimental Music Technologies Lab, which represents a diverse group of researchers and practicing musicians at University of Sussex.&lt;/p&gt;

&lt;p&gt;Please contact  Dr Chris Kiefer, c.kiefer@sussex.ac.uk for informal enquiries.&lt;/p&gt;

</description>
        <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/MEML-PDRA</link>
        <guid isPermaLink="true">http://localhost:4000/blog/MEML-PDRA</guid>
        
        <category>MEML</category>
        
        <category>jobs</category>
        
        <category>research fellow</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Emute Lab @ CHIME 2023 Workshop</title>
        <description>&lt;p&gt;Emute Lab members Steve Symons, Dimitris Kyriakoudis and Chris Kiefer presented their work at the &lt;a href=&quot;https://www.chime.ac.uk/chime-annual-workshop&quot;&gt;CHIME annual workshop&lt;/a&gt; in December, &lt;i&gt;‘a one-day research event for sharing work in the area of music and human-computer interaction, with a particular emphasis on participative hands-on demos’&lt;/i&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://entangled-instruments.xyz/wp-content/uploads/2023/07/playingStickatron.png&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Steve presented his work on entangled instruments &lt;a href=&quot;https://entangled-instruments.xyz&quot;&gt;https://entangled-instruments.xyz/&lt;/a&gt;, and also took part in the afternoon panel session.&lt;/p&gt;

&lt;div id=&quot;pdfembed0&quot; style=&quot;width:100%; height:800px;&quot;&gt;&lt;/div&gt;
&lt;script src=&quot;https://cdn.jsdelivr.net/npm/pdfobject@2.2.8/pdfobject.min.js&quot; crossorigin=&quot;anonymous&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/script&gt;

&lt;script&gt;PDFObject.embed(&quot;https://www.emutelab.org/docs/CHIMEAbstractSymons.pdf&quot;, &quot;#pdfembed0&quot;, {height: &quot;100%&quot;});&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
Slides:&lt;/p&gt;

&lt;div id=&quot;pdfembed1&quot; style=&quot;width:100%; height:800px;&quot;&gt;&lt;/div&gt;
&lt;script src=&quot;https://cdn.jsdelivr.net/npm/pdfobject@2.2.8/pdfobject.min.js&quot; crossorigin=&quot;anonymous&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/script&gt;

&lt;script&gt;PDFObject.embed(&quot;https://www.emutelab.org/docs/sharedOrEntangled.pdf&quot;, &quot;#pdfembed1&quot;, {height: &quot;100%&quot;});&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Dimitris presented his work on his livecoding instrument Timelines.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lnfiniteMonkeys/TimeLines-hs&quot;&gt;https://github.com/lnfiniteMonkeys/TimeLines-hs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;!-- &lt;a href=&quot;https://www.chime.ac.uk/s/CHIME-Abstract-Kyriakoudis.pdf&quot;&gt;https://www.chime.ac.uk/s/CHIME-Abstract-Kyriakoudis.pdf&lt;/a&gt; --&gt;&lt;/p&gt;

&lt;div id=&quot;pdfembed2&quot; style=&quot;width:100%; height:800px;&quot;&gt;&lt;/div&gt;
&lt;script src=&quot;https://cdn.jsdelivr.net/npm/pdfobject@2.2.8/pdfobject.min.js&quot; crossorigin=&quot;anonymous&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/script&gt;

&lt;script&gt;PDFObject.embed(&quot;https://www.emutelab.org/docs/CHIMEAbstractKyriakoudis.pdf&quot;, &quot;#pdfembed2&quot;, {height: &quot;100%&quot;});&lt;/script&gt;

&lt;p&gt;Slides:&lt;/p&gt;

&lt;div id=&quot;pdfembed3&quot; style=&quot;width:100%; height:800px;&quot;&gt;&lt;/div&gt;
&lt;script src=&quot;https://cdn.jsdelivr.net/npm/pdfobject@2.2.8/pdfobject.min.js&quot; crossorigin=&quot;anonymous&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/script&gt;

&lt;script&gt;PDFObject.embed(&quot;https://www.emutelab.org/docs/DK_CHIME2023.pdf&quot;, &quot;#pdfembed3&quot;, {height: &quot;100%&quot;});&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
Chris showed his new instrument, the Nalima, which explores multistable musicianship.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/y3uq0skcnB0?si=9ksfu9smmzt5iKZv&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;div id=&quot;pdfembedCK&quot; style=&quot;width:100%; height:800px;&quot;&gt;&lt;/div&gt;
&lt;script src=&quot;https://cdn.jsdelivr.net/npm/pdfobject@2.2.8/pdfobject.min.js&quot; crossorigin=&quot;anonymous&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/script&gt;

&lt;script&gt;PDFObject.embed(&quot;https://www.emutelab.org/docs/CHIMEAbstractKiefer.pdf&quot;, &quot;#pdfembedCK&quot;, {height: &quot;100%&quot;});&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
Some images of the event:
&lt;br /&gt;
&lt;img src=&quot;../img/chime2023/PXL_20231204_153446625.jpg&quot; /&gt;
&lt;br /&gt;
Dimitris’ presentation
&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;../img/chime2023/PXL_20231204_154100975.MP.jpg&quot; /&gt;
&lt;br /&gt;
Steve demos the Stickatron
&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;../img/chime2023/PXL_20231204_164043686.MP.jpg&quot; /&gt;
&lt;br /&gt;
Trying out Chris’ Nalima instrument
&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;../img/chime2023/PXL_20231204_172352560.MP.jpg&quot; /&gt;
&lt;br /&gt;
Steve’s presentation for the afternoon panel
&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 04 Dec 2023 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/CHIME2023</link>
        <guid isPermaLink="true">http://localhost:4000/blog/CHIME2023</guid>
        
        <category>music</category>
        
        <category>HCI</category>
        
        <category>entangled</category>
        
        <category>multistable</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>New Emute Lab Mailing List</title>
        <description>&lt;p&gt;We have a new email list, please sign up here for news and discussion:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.jiscmail.ac.uk/cgi-bin/webadmin?SUBED1=EMUTELAB&amp;amp;A=1&quot;&gt;https://www.jiscmail.ac.uk/cgi-bin/webadmin?SUBED1=EMUTELAB&amp;amp;A=1&lt;a&gt;&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Fri, 24 Nov 2023 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/newmailinglist</link>
        <guid isPermaLink="true">http://localhost:4000/blog/newmailinglist</guid>
        
        <category>email</category>
        
        <category>mailing list</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Emute Lab @ Machina Bristronica</title>
        <description>&lt;p&gt;Emute Lab are at Machina Bristronica - Chris and Dimitris are showing the new uSEQ livecoding module, and Steve Symons is showing his system for interfacing eurorack with wireless gestural controllers.&lt;/p&gt;

&lt;p&gt;Links:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lnfiniteMonkeys/uSEQ&quot;&gt;https://github.com/lnfiniteMonkeys/uSEQ&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.muio.org/&quot;&gt;http://www.muio.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/bristronica.webp&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/bristronica/1.jpg&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/bristronica/2.jpg&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/bristronica/3.jpg&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/bristronica/4.jpg&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/bristronica/5.jpg&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/bristronica/6.jpg&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/bristronica/7.jpg&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/bristronica/8.jpg&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 30 Sep 2023 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/Bristronica</link>
        <guid isPermaLink="true">http://localhost:4000/blog/Bristronica</guid>
        
        <category>eurorack</category>
        
        <category>livecoding</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Post-AIMC 2023 Reflection</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://aimc2023.pubpub.org&quot;&gt;&lt;img src=&quot;/img/aimclogo.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;reflecting-on-aimc-2023&quot;&gt;Reflecting on AIMC 2023&lt;/h3&gt;

&lt;p&gt;In September 2023 two research labs focussing on musical instruments and live performance came together to run the &lt;strong&gt;International Conference on Artificial Intelligence and Music Creativity&lt;/strong&gt;, or &lt;a href=&quot;https://aimc2023.pubpub.org&quot;&gt;AIMC 2023&lt;/a&gt;. After our application to the &lt;a href=&quot;https://aimusiccreativity.org&quot;&gt;AI and Music Creativity&lt;/a&gt; board, it was decided that we would run the 2023 conference at the University of Sussex with the theme of “Intelligent Performance Systems”.&lt;/p&gt;

&lt;p&gt;The two labs organising the conference were:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.emutelab.org&quot;&gt;Experimental Music Technologies Lab&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.iil.is&quot;&gt;Intelligent Instruments Lab&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The conference had 75 submissions from all over the world across diverse tracks (papers, demos, doctoral consortium and performances). We also had 14 Alt-AIMC submissions, but the idea with the “Alt” is to allow for late breaking, unfinished or controversial work. Those were judged by the jury and not double blind peer-reviewed like the academic papers. 85 people registered to the conference and in addition to the Sussex and Icelandic teams, we were around 100 people participating.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;conference-format-and-proceedings&quot;&gt;Conference format and Proceedings&lt;/h3&gt;

&lt;p&gt;The conference had two keynotes, paper sessions (5 min lightning talks, followed by poster sessions), performances, workshops, demos and an algorave.&lt;/p&gt;

&lt;p&gt;Check the website here: &lt;a href=&quot;https://aimc2023.pubpub.org&quot;&gt;AIMC 2023&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The AIMC 2023 proceedings can be found &lt;a href=&quot;http://www.aimc2023.org/proceedings&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;streamed-video-recordings-of-all-sessions&quot;&gt;Streamed Video Recordings of all sessions&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Keynote 1 - Dadabots&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CJ gave a keynote on neural synthesis and the training of audio diffusion models.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/hsVMs2k0vck?si=-GNOxVGSZLhjmJh_&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Keynote 2 - Elaine Chew&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Elaine Chew, who runs the &lt;a href=&quot;https://cosmos.isd.kcl.ac.uk&quot;&gt;COSMOS project&lt;/a&gt; presented her past work in the field of computational analysis of musical creativity.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Jtb6tYH4XaE?si=jLFrDTecpTi6Scjq&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Elaine wrote a &lt;a href=&quot;https://cosmos.isd.kcl.ac.uk/?p=4286&quot;&gt;blog post&lt;/a&gt; about her participation at AIMC.&lt;/p&gt;

&lt;p&gt;Information on both keynote speakers can be found &lt;a href=&quot;https://aimc2023.pubpub.org/keynotes&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Industry Panel&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.olliebown.com&quot;&gt;Ollie Bown&lt;/a&gt; organised an industry panel with &lt;a href=&quot;https://dadabots.com&quot;&gt;CJ of Dadabots&lt;/a&gt;, &lt;a href=&quot;https://samim.io&quot;&gt;Samim Winiger&lt;/a&gt;, &lt;a href=&quot;https://www.kcl.ac.uk/people/elaine-chew&quot;&gt;Elaine Chew&lt;/a&gt;, and &lt;a href=&quot;https://www.ryangroves.com&quot;&gt;Ryan Groves&lt;/a&gt;.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Dat1P9oa5QE?si=edP3KjZbLyrrwcKM&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Paper session 1 (incl. Alt-AIMC and Demos)&lt;/strong&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/xmRRu5yfCF8?si=re1Pf-iRelwZMtJt&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Main Papers/Posters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“Bela-IREE: An Approach to Embedded Machine Learning for Real-Time Music Interaction” Ezra Pierce, Victor Shepardson, Jack Armitage, Thor Magnusson&lt;/p&gt;

&lt;p&gt;“[neuralnet]: A Pure Data External for the Creation of Neural Networks Written in Pure C” Alexandros Drymonitis&lt;/p&gt;

&lt;p&gt;“NeuralMidiFx: A Wrapper Template for Deploying Neural Networks as VST3 Plugins” Behzad Haki, Julian Lenz, Sergi Jorda&lt;/p&gt;

&lt;p&gt;“Exploring Latent Spaces of Tonal Music using Variational Autoencoders” Nádia Carvalho, Gilberto Bernardes de Almeida&lt;/p&gt;

&lt;p&gt;“Emotional Machines” Jorge Forero, Mónica Mendes, Gilberto Bernardes&lt;/p&gt;

&lt;p&gt;“Finetuning Rolypoly~ 2.0: an expressive drum machine that adapts with every performance” Grigore Burloiu&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Alt-AIMC Papers/Posters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“AI in live coding environments: Pandora’s Dream” Celeste Betancur Gutierrez&lt;/p&gt;

&lt;p&gt;“Latent Space Explorer” Alexander Lunt, Sebastian Trump&lt;/p&gt;

&lt;p&gt;“Investigation of Live Coding Using a Combination of ChatGPT and Fine-Tuned GPT-3” Tomoki Okuda and  Kazuhiro Jo&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Paper session 2 (incl. Alt-AIMC and Demos)&lt;/strong&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/zFYlFZh1sw0?si=qTSbjzw4NqT-8-yz&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main Papers/Posters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“The Phenomenology of Deconstructivist Aesthetics in Music: An Autoethnography of Errors, Erasures, Permutations, Discontinuities, Paradoxes and Artificial Intelligences” Philon Nguyen, Eldad Tsabary&lt;/p&gt;

&lt;p&gt;“Music AI’s Potential Impact: Scoping the terms of the debate about value” Oliver Bown&lt;/p&gt;

&lt;p&gt;“Caring Trouble and Musical AI: Considerations towards a Feminist Musical AI” Kelsey Cotton, Kıvanç Tatar&lt;/p&gt;

&lt;p&gt;“YouTube Mirror: An Interactive Audiovisual Installation based on Cross-Modal Generative Modeling” Sihwa Park&lt;/p&gt;

&lt;p&gt;“Risks and Opportunities from Artificial Creativity” Roisin Loughran&lt;/p&gt;

&lt;p&gt;“Extensible Embodied Knowledge: Bridging Performance Practice and Intelligent Performance System Design” Lucy Strauss, Matthew Yee-King&lt;/p&gt;

&lt;p&gt;“The A in AIMC” Thor Magnusson&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Alt-AIMC Papers/Posters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“Are we solving the wrong problems - and doing harm in the process?” Anna-Kaisa Kaila, Andre Holzapfel, Bob L. T. Sturm&lt;/p&gt;

&lt;p&gt;“Beyond mutation: how can we acknowledge symbiogenesis in evolutionary music coding?” Matthias Jung&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Paper session 3 (incl. Alt-AIMC and Demos)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/eTHkk1eT7I4?si=bf8pul0sUzFE9iI8&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main Papers/Posters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“Musical and Meta-Musical Conversations” Oded Ben-Tal, David Dolan&lt;/p&gt;

&lt;p&gt;“Silicon for Orchestra and Artificial Intelligence: Three Strategies for Incorporating Artificial Intelligence into the Compositional Process of Orchestral Music” Robert Laidlow&lt;/p&gt;

&lt;p&gt;“Human-AI Musicking: A Framework for Designing AI for Music Co-creativity” Craig Vear, Steve Benford, Juan Martinez Avila, Solomiya Moroz&lt;/p&gt;

&lt;p&gt;“Revisiting Reynolds - Autonomous Agents for Spatial Audiovisual Composition and Performances” Damian Dziwis&lt;/p&gt;

&lt;p&gt;“Liveness and machine listening in musical live coding: A conceptual framework for designing agent-based systems” Georgios Diapoulis&lt;/p&gt;

&lt;p&gt;“Virtual AI Jam: AI-Driven Virtual Musicians for Human-in-the-Loop Musical Improvisation” Torin Hopkins, Alvin Jude, Greg Phillips, Ellen Do&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Alt-AIMC Papers/Posters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“Introducing the Caulsio: using causal inference to influence collaboration in a shared music making environment” Steve Symons&lt;/p&gt;

&lt;p&gt;“Accessible Co-Creativity through Language and Voice Input” Prateek Verma, Constantin Basica, Patricia Alessandrini, Alexandru Berceanu&lt;/p&gt;

&lt;p&gt;“Intimate Musical Collaboration with a Probabilistic Model” Karl Johannsson&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Paper session 4 (incl. Alt-AIMC and Demos)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/1oOr3EnMn_o?si=jOC-hTef1zqAzg5x&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main Papers/Posters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“Sequential Pitch Distributions for Raga Detection” VISHWAAS NARASINH, SENTHIL RAJA G&lt;/p&gt;

&lt;p&gt;“The Ai Music Generation Challenge 2022: Summary and Results” Bob L. Sturm&lt;/p&gt;

&lt;p&gt;“Statistical evaluation of abc-formatted music at the levels of items and corpora” Laura Cros Vila, Bob L. T. Sturm&lt;/p&gt;

&lt;p&gt;“Parsing musical structure to enable meaningful variations” Maziar Kanani, Seán O’Leary, James McDermott&lt;/p&gt;

&lt;p&gt;“Deep Learning with Audio: An Explorative Syllabus for Music Composition and Production” Koray Tahiroglu, Shenran Wang, Eduard Mihai Tampu, Jackie Lin&lt;/p&gt;

&lt;p&gt;“Are words enough? On the semantic conditioning of affective music generation.” Jorge Forero, Gilberto Bernardes, Mónica Mendes&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Alt-AIMC Papers/Posters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“Closing the Loop: Enabling User Feedback and Testing in Symbolic Music Generation through a Python Framework and Ableton Live Integration” Rui Guo&lt;/p&gt;

&lt;p&gt;“Introductory Studies on Raga Multi-track Music Generation of Indian classical music using AI” Sreekanth Gopi, Femi William&lt;/p&gt;

&lt;p&gt;“Building a Nature Soundscape Generator for the Post-Biodiversity Future” Avery Bick&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Closing Session&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Thor Magnusson and Chris Kiefer reflecting upon the conference and Oded Ben-Tal introduces AIMC 2024 in Oxford.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/yC1PvuW2EWQ?si=hYqFBFNyP-qKpCEg&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Concert 1 in ACCA&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/EZEnqwB1b2w?si=_vZFH1DRMLKCiffi&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Odd Couple&lt;/em&gt;, Oded Ben-Tal and David Dolan, piano and computer.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Organic Algorithmic Composition&lt;/em&gt;, Gyuchul Moon.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Quantum Fantasy&lt;/em&gt;, Jeff Morris. For Conductor and Computer.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;AI&lt;/em&gt;, Franziska Schroeder and Federico Rueben. Saxophone and live coding.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Concert 2 in ACCA&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/xpN5Bxvkq08?si=WMbaQJczMJ_u1tez&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;One, Two, Many&lt;/em&gt;, Oded Ben-Tal. For two flutes and computer. Rowland Sutherland on flute 1.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;iː ɡoʊ weɪ&lt;/em&gt;, by Jonathan Reus. For Computer and Voice.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notochord Arcs and scrambled Signals&lt;/em&gt;, Victor Shepardson and Nicola Privato. For two computers and human operators.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;SCAMP Singularity&lt;/em&gt;, Henrique Portovedo. Augmented Saxophone and Computer.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The historically Informed AI: Johann Sperger through Time and Bass, for Contrabass, Viola, Magnetic Discs and Computer. Based on Sonata for Contrabass and Viola by Johann Matthias Sperger (1777)&lt;/em&gt;. Darija Andzakovic , Natalia Duarte, and Nicola Privato.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Algorave in Brighton&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/d0RMUqcbhmQ?si=mwIWSFH21HQFXtjQ&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Uncanny&lt;/em&gt;, Begüm Çelik and Tuğrul Veli Şalcı&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cartographic&lt;/em&gt;, Tasos Asonitis&lt;/p&gt;

&lt;p&gt;&lt;em&gt;James GM (Live) - Machine Learning And Audio-Visual Performance In Unreal Engine 5&lt;/em&gt;, James Gibbons-MacGregor&lt;/p&gt;

&lt;p&gt;&lt;em&gt;drum.code&lt;/em&gt;, Timo Hoogland&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Pandora’s Dream&lt;/em&gt;, Celeste Betancur Gutierrez&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Live Coding with an Affective Autonomous Agent in TidalCycles- Performance&lt;/em&gt;, Liz Wilson&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Gagnavera&lt;/em&gt;, Jack Armitage&lt;/p&gt;

</description>
        <pubDate>Tue, 05 Sep 2023 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/postAIMC2023</link>
        <guid isPermaLink="true">http://localhost:4000/blog/postAIMC2023</guid>
        
        <category>emute</category>
        
        <category>ii lab</category>
        
        <category>intelligent</category>
        
        <category>AI</category>
        
        <category>creativeAI</category>
        
        
      </item>
    
      <item>
        <title>New Release on Emute: Live at the Meeting House</title>
        <description>&lt;p&gt;Full details on the &lt;a href=&quot;/label/fmn_liveathemeetinghouse&quot;&gt;release page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;/label/fmn_liveathemeetinghouse&quot;&gt;&lt;img src=&quot;/img/FMN_LATMH_flyer_small.png&quot; width=&quot;50%&quot; /&gt;&lt;/a&gt;


&lt;/p&gt;
</description>
        <pubDate>Fri, 03 Mar 2023 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/FMN_Meeting-House</link>
        <guid isPermaLink="true">http://localhost:4000/blog/FMN_Meeting-House</guid>
        
        <category>feedback</category>
        
        <category>emute</category>
        
        <category>label</category>
        
        <category>recording</category>
        
        
        <category>blog</category>
        
      </item>
    
  </channel>
</rss>
